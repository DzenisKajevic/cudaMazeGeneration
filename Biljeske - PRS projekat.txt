Prvobitna razmisljanja - svaki thread treba da ciljano generise dio jednog velikog labirinta sa poznatom tackom ulaza i izlaza - nemoguce:

Forward Backtracking - DFS (stara razmisljanja):
1 - Nemoguce paralelizovati direktno - Mora ici do celije koja nema susjeda, vratiti se unazad itd.
2 - Nemoguce podijeliti labirint na n jednakih dijelova pa na svakom pokrenuti direktno algoritam jer ne znamo kuda bi trebala prolaziti putanja
3 - Potencijalno moguce randomly generisati indekse jednakih dijelova labirinta kroz koje ce prolaziti putanja pa na osnovu toga generisati temporary ulaze i izlaze i pokrenuti algoritam. Na kraju spojiti glavni ulaz i finalni izlaz

Pravo rjesenje: 
 - Tacka izlaza nije potrebna za generisanje labirinta - ona se odabere nakon sto je labirint napravljen.
 - Svaki thread generise svoj odvojeni labirint. Kasnije je potrebno spojiti te labirinte u jednu cjelinu
 - S obzirom da su individualni generisani labirinti "perfektni", tj. iz svake celije je moguce doci u bilo koju drugu tacno jednom putanjom, trebalo bi ih biti moguce spojiti na taj nacin  da je opet moguce iz svake celije doci u bilo koju drugu na samo jedan nacin ---- Ispostavilo se da je to priblizno onom sto se uspjelo uraditi, ali mislim da nije moguce da bude  perfetkno.  Postoji dobra sansa da u svakom generisanom labirintu bude par mogucih putanja, ali ih nikad nece biti previse.
 - Potrebno generisati PxP labiranata (threadova) sa NxN velicinama labirinta. Velicina finalnog (spojenog) labirinta je AxB x AxB - tj. ako imamo 4x4 threadova (16) koji generisu 21x21  labirinte, velicina finalnog labirinta je 4x21 x 4x21 = 7056
 - Svaki interni labirint mora biti razlicit (nasumican) - da bi se to postiglo, CUDA ima svoju funkciju - curand_init(), koja inicijalizira random stanja (curandState) za svaki thread. 

Prva zamisao pravog rjesenja:
 - Generisati labirinte bez granica da bi ih bilo lakse spojiti + kasnije dodati glavne granice (okvir labirinta):
  + Tesko za izvesti 
  + Nakon spajanja, rezultati najvjerovatnije ne bi bili dobri, bilo bi previse konekcija izmedju malih labirinata, samim time bi bilo i previse mogucih putanja

_____________


- "The number of threads launched should be a perfect square to allow for an even grid arrangement of the smaller mazes."
- "First, determine the size of the smaller mazes and how many of them will fit into the larger maze. For simplicity, let's assume each smaller maze is of size n x n, and you want to create a grid of p x p smaller mazes, resulting in a larger maze of size (p*n) x (p*n)."


- """To achieve the goal of generating individual mazes in parallel on the GPU and then combining them into a larger maze, you can follow these steps:

 1. Define Maze Dimensions
 First, determine the size of the smaller mazes and how many of them will fit into the larger maze. For simplicity, let's assume each smaller maze is of size n x n, and you want to create a  grid of p x p smaller mazes, resulting in a larger maze of size (p*n) x (p*n).

 2. Initialize GPU Memory
 You need to allocate memory for each thread's maze on the GPU. Each thread will have its own portion of the memory to write its maze.

 3. Maze Generation Kernel
 Each GPU thread will generate its own maze. The maze generation logic from your existing CPU code will be adapted for the GPU. This involves initializing each maze, generating the paths,  and ensuring that each thread works independently.

 4. Combine Mazes
 Once all threads have completed their maze generation, you will need to combine the smaller mazes into a larger one. This could either be done on the GPU or the CPU, depending on where you  want to manage the final maze.
"""


- """Array Dimensions: For simplicity, we assume the maze size is fixed as N to avoid dynamic memory allocation. If you need dynamic sizes, you'd have to manage memory differently.
Maze Storage: The maze is stored in a 1D array, so indexing uses row * size + col.
Randomness: The use of curand in CUDA ensures that each thread can generate its own random values.
Performance Consideration: The DFS and backtracking are inherently sequential. While CUDA allows you to generate multiple mazes in parallel, the generation of each maze itself is sequential.
"""

___________________________________________________

SPAJANJE LABIRINATA (PARALELNA VERZIJA):
"""
Let's break down the GPU combination process in detail, focusing on how the number of threads and the grid dimensions are set up.

1. Understanding the Problem:
You have multiple small mazes (P x P), each of size N x N. You want to combine these small mazes into one large maze of size (N * P) x (N * P). This large maze is stored in a flattened array on the GPU.

2. Setting Up the Grid and Block Dimensions:
CUDA programming involves launching a grid of thread blocks, where each block contains multiple threads. For your problem:

Block Dimensions (blockDim): This defines how many threads will be in each block. Since each small maze is N x N, we can use a block of size N x N, meaning each thread in a block corresponds to a single cell in the small maze.

Grid Dimensions (gridDim): This defines how many blocks will be in the grid. Since there are P x P small mazes, you need P x P blocks to cover all small mazes.

Additionally, each block processes a specific small maze, so the grid is 3D:

P x P blocks in the x and y dimensions correspond to the P x P small mazes.
The third dimension (z) of the grid is used to loop over all small mazes.
3. Grid and Block Setup in the Code:
Hereâ€™s the relevant code snippet:

cpp
Copy code
// Define block and grid dimensions for combining mazes
dim3 blockDim(N, N, 1);  // Each block has N x N threads
dim3 gridDim(P, P, num_mazes);  // Grid has P x P blocks, with num_mazes in the third dimension
4. Understanding the Kernel Execution:
The kernel combine_mazes is launched with the above blockDim and gridDim settings.

Kernel Execution:
cpp
Copy code
__global__ void combine_mazes(MAZE_PATH* small_mazes, MAZE_PATH* large_maze, int small_size, int large_size, int num_mazes_per_side) {
    int small_row = blockIdx.y * blockDim.y + threadIdx.y;
    int small_col = blockIdx.x * blockDim.x + threadIdx.x;
    int maze_index = blockIdx.z;

    if (small_row < small_size && small_col < small_size) {
        int large_row = (maze_index / num_mazes_per_side) * small_size + small_row;
        int large_col = (maze_index % num_mazes_per_side) * small_size + small_col;

        large_maze[large_row * large_size + large_col] = small_mazes[maze_index * small_size * small_size + small_row * small_size + small_col];
    }
}
Explanation:
Index Calculations:

blockIdx.x and blockIdx.y: These represent the coordinates of the block within the grid. For example, if P=4, the grid is 4x4, and blockIdx.x ranges from 0 to 3.
threadIdx.x and threadIdx.y: These represent the threadâ€™s position within the block, used to determine the specific cell within the small maze.
blockIdx.z: This indicates the index of the small maze being processed by the block.
Mapping to Large Maze:

The small_row and small_col are derived from the thread indices, corresponding to the position within the small maze.
The large_row and large_col map the small mazeâ€™s cell to the correct position in the large maze.
Combining Process:

For each block (which corresponds to a small maze), the threads (which correspond to cells in the small maze) place the cells in the appropriate location within the large maze.
5. Summary of GPU Combination:
Blocks and Threads:

Each N x N block handles a N x N small maze.
The grid is P x P blocks wide and tall, with an additional dimension in the z direction to cover all the small mazes.
Advantages:

This setup ensures that the GPU efficiently combines all small mazes into one large maze in parallel, with each thread handling one cell.
Efficiency:

This method leverages the GPUâ€™s parallel processing power to perform the combination much faster than it could be done sequentially on the CPU.
"""

____________________________________________________________


Union-Find struktura podataka - koristi subsets da zna kad su interni labirinti u istom podskupu. Ukoliko su u istom podskupu, ne treba praviti konekciju izmedju njih. Ovo se koristi da bi se pokusala osigurati samo jedna putanja izmedju 2 tacke u labirintu, tj. pokusavamo sprijeciti cikluse unutar finalnog labirinta. 
Ovo bi u potpunosti ispunjavalo taj cilj, da nije problem sa spajanjem labirinata, tj. kada se spoje dva interna labirinta, postoji sansa da interni zidovi blokiraju tu vezu. Naknadno je napisan kod koji uklanja te interne celije (ukoliko su prisutne), sto povecava sansu da ima vise putanja do iste celije (nije idealno).
Ukoliko je veza izmedju dva labirinta horizontalna (labirint A je lijevo od labirinta B, pored 2 granicne celije (zida) koje se nalaze izmedju njih, dodatno se brise po jedna celija lijevo, odnosno desno od konekcije, zavisno u kojem se labirintu nalazimo.
Isto tako i za vertikalne veze - po jedna celija iznad / ispod mjesta konekcije, se brise...


"""
The Union-Find data structure, also known as Disjoint Set Union (DSU) or Disjoint Set, is a data structure that keeps track of a set of elements partitioned into non-overlapping (disjoint) subsets. It is particularly useful for problems involving connectivity, such as determining whether two elements are in the same subset or merging two subsets into one.

Key Concepts of Union-Find
Union: This operation merges two subsets into a single subset.
Find: This operation determines which subset a particular element belongs to. It can be used to check if two elements are in the same subset.
Structure
Union-Find is typically implemented with two main components:

Parent Array: This array represents the parent or leader of each element. If an element is its own parent, it is the leader (or root) of its subset.
Rank Array (or Size Array): This is an optional array used to keep the tree flat. It stores the "rank" or "size" of each subset. The idea is to attach the smaller tree under the root of the larger tree to keep the structure balanced, which helps optimize the performance of the union-find operations.
Operations
1. Find Operation
The find operation returns the root of the subset to which an element belongs. During this operation, we perform "path compression," where we make the trees flatter by pointing all nodes directly to the root.

cpp
Copy code
int find(int x) {
    if (parent[x] != x) {
        parent[x] = find(parent[x]); // Path compression
    }
    return parent[x];
}
Path Compression: This technique flattens the structure of the tree whenever find is called, making future operations faster.
2. Union Operation
The union operation merges two subsets into one. This is typically done by attaching the root of one tree under the root of another, preferably the smaller tree under the larger to maintain balance.

cpp
Copy code
bool union_sets(int x, int y) {
    int rootX = find(x);
    int rootY = find(y);

    if (rootX != rootY) {
        if (rank[rootX] > rank[rootY]) {
            parent[rootY] = rootX;
        } else if (rank[rootX] < rank[rootY]) {
            parent[rootX] = rootY;
        } else {
            parent[rootY] = rootX;
            rank[rootX]++;
        }
        return true;
    }
    return false; // Already in the same set
}
Union by Rank: This technique ensures that the smaller tree is always attached under the root of the larger tree to keep the tree balanced and the depth as shallow as possible.
Efficiency
Time Complexity: Both the find and union operations have nearly constant time complexity, 
ð‘‚
(
ð›¼
(
ð‘›
)
)
O(Î±(n)), where 
ð›¼
Î± is the inverse Ackermann function, which grows very slowly. For all practical purposes, these operations are considered to run in constant time.
Application in Maze Connectivity
In our context, we are using the Union-Find structure to ensure that when connecting two mazes:

Avoid Cycles: We only connect two mazes if they are not already connected, which is checked using the find operation.
Maintain a Single Connected Component: By connecting mazes without forming cycles, we ensure that the resulting maze remains a "perfect maze" where there is only one path between any two points.
Why Union-Find?
When connecting mazes, we want to ensure that we don't accidentally create multiple paths between any two points, which would violate the "perfect maze" property. Union-Find helps us track which mazes are already connected, so we can carefully manage the connections to maintain a single path between any two points.
"""

________________________________________________

BLOCK SIZES FOR 101x101 MAZES:


Let's break down the comparison between using a 16x16 block size and a 32x32 block size on a grid of size 101x101.

Grid and Block Configuration
For 16x16 Block Size:
Grid dimensions:
gridX = (101 + 16 - 1) / 16 = 7
gridY = (101 + 16 - 1) / 16 = 7
The grid is 7x7 blocks.
Thread Utilization:
Each block has 256 threads (16x16).
The grid covers a 112x112 area, with the last block in each dimension (x and y) having extra threads that don't correspond to any maze cells.
Overhead:
The last block will have 15 extra threads in each dimension that do not perform any useful work (15x15=225 unused threads per block in the corner).
For 32x32 Block Size:
Grid dimensions:

gridX = (101 + 32 - 1) / 32 = 4
gridY = (101 + 32 - 1) / 32 = 4
The grid is 4x4 blocks.
Thread Utilization:

Each block has 1024 threads (32x32).
The grid covers a 128x128 area, with the last block in each dimension (x and y) having extra threads that don't correspond to any maze cells.
Overhead:
The last block will have 31 extra threads in each dimension that do not perform any useful work (31x31=961 unused threads per block in the corner).
Overhead Comparison
16x16 block size:
Each block can have up to 225 unused threads in the worst-case scenario (the corner block).
The overall thread count is 7x7x256 = 12,544 threads, with a higher proportion of active threads.
32x32 block size:
Each block can have up to 961 unused threads in the worst-case scenario (the corner block).
The overall thread count is 4x4x1024 = 16,384 threads, with a higher proportion of inactive threads in the last block.
Advantages and Disadvantages
16x16 Block Size:
Advantages:

Better Thread Utilization:
Less thread wastage per block, especially for grids with dimensions not divisible by 32.
Flexibility:
Smaller blocks can provide better flexibility for covering the grid with minimal wastage.
Disadvantages:

Higher Grid Size:
More blocks are required to cover the entire grid, which may increase the overhead in terms of block scheduling and memory accesses.
32x32 Block Size:
Advantages:

Lower Grid Size:
Fewer blocks are needed to cover the grid, which can reduce overhead related to block scheduling.
Memory Access Coalescing:
Larger blocks may provide better memory access patterns, leading to improved coalescing for certain memory operations, assuming alignment.
Disadvantages:

Higher Thread Wastage:
Significant thread wastage for non-divisible grid sizes, leading to higher overhead per block, especially in the corner blocks.
Potential for Warp Divergence:
With more threads per block, there's a higher chance for divergence if the threads within a warp execute different paths of code.
Which One is Better Overall?
For N = 101, the 16x16 block size might be better overall due to lower thread wastage, especially when the grid size is not a perfect multiple of 32. The increased number of blocks (7x7 vs. 4x4) introduces some additional overhead in terms of scheduling, but this is typically outweighed by the better thread utilization.
32x32 block size is still a valid choice if the problem is compute-bound and memory access coalescing is critical. However, for grids where the dimensions are not multiples of 32, the thread wastage could reduce overall efficiency.
Conclusion:
For N = 101, 16x16 is likely the better choice because it provides better thread utilization with minimal overhead, even though it requires more blocks to cover the grid. The reduced thread wastage outweighs the additional scheduling overhead in most scenarios.